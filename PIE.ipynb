{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o--3yDqTuTH"
      },
      "source": [
        "# DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEO2_ulVdYE_",
        "outputId": "8ef163a1-51e9-414d-bdb7-04592e9b7983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# @title import\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import json\n",
        "import numpy as np\n",
        "import time\n",
        "import cv2\n",
        "import sys\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "from os import makedirs, listdir\n",
        "from os.path import join, abspath, isfile, isdir\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from prettytable import PrettyTable\n",
        "import PIL\n",
        "from PIL import Image, ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import random\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.applications import vgg16\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.layers import Concatenate, LSTM, ConvLSTM2D, Dense, Flatten, Input, RepeatVector\n",
        "from keras.models import Model, load_model\n",
        "from keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "cellView": "form",
        "id": "z16sG6S4YVir"
      },
      "outputs": [],
      "source": [
        "# @title data_preparation\n",
        "\n",
        "class PIE(object):\n",
        "    def __init__(self, regen_database=False, data_path=''):\n",
        "        \"\"\" Class constructor\n",
        "            :param regen_database: Whether generate the database or not\n",
        "            :param data_path: The path to wh \"\"\"\n",
        "        self._name = 'pie'\n",
        "        self._image_ext = '.png'\n",
        "        self._regen_database = regen_database\n",
        "        # Paths\n",
        "        self._pie_path = data_path if data_path else self._get_default_path()\n",
        "        assert isdir(self._pie_path), \\\n",
        "            'pie path does not exist: {}'.format(self._pie_path)\n",
        "        self._annotation_path = join(self._pie_path, 'annotations')\n",
        "        self._annotation_attributes_path = join(self._pie_path, 'annotations_attributes')\n",
        "        self._annotation_vehicle_path = join(self._pie_path, 'annotations_vehicle')\n",
        "        self._clips_path = join(self._pie_path, 'PIE_clips')\n",
        "        self._images_path = join(self._pie_path, 'images')\n",
        "\n",
        "    # Path generators\n",
        "    @property\n",
        "    def cache_path(self):\n",
        "        \"\"\" Generates a path to save cache files \"\"\"\n",
        "        cache_path = abspath(join(self._pie_path, 'data_cache'))\n",
        "        if not isdir(cache_path):\n",
        "            makedirs(cache_path)\n",
        "        return cache_path\n",
        "    def _get_default_path(self):\n",
        "        \"\"\" Returns the default path where pie is expected to be installed. \"\"\"\n",
        "        return 'data/pie'\n",
        "    def _get_image_set_ids(self, image_set):\n",
        "        \"\"\" Returns default image set ids\n",
        "                :param image_set: Image set split\n",
        "                :return: Set ids of the image set  \"\"\"\n",
        "\n",
        "        image_set_nums = {'train': ['set01'],\n",
        "                          'val': ['set02'],\n",
        "                          'test': ['set03'],\n",
        "                          'all': ['set01', 'set02', 'set03']}\n",
        "\n",
        "        return image_set_nums[image_set]\n",
        "\n",
        "\n",
        "    def _get_image_path(self, sid, vid, fid):\n",
        "        \"\"\" Generates and returns the image path given ids\n",
        "              :param sid: Set id\n",
        "              :param vid: Video id\n",
        "              :param fid: Frame id\n",
        "              :return: Return the path to the given image \"\"\"\n",
        "        return join(self._images_path, sid, vid, '{:05d}.png'.format(fid))\n",
        "\n",
        "    # Visual helpers\n",
        "    def update_progress(self, progress):\n",
        "        \"\"\" Creates a progress bar \"\"\"\n",
        "        barLength = 20\n",
        "        status = \"\"\n",
        "        if isinstance(progress, int):\n",
        "            progress = float(progress)\n",
        "        block = int(round(barLength * progress))\n",
        "        text = \"\\r[{}] {:0.2f}% {}\".format(\"#\" * block + \"-\" * (barLength - block), progress * 100, status)\n",
        "        sys.stdout.write(text)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    def _print_dict(self, dic):\n",
        "        \"\"\" Prints a dictionary, one key-value pair per line \"\"\"\n",
        "        for k, v in dic.items():\n",
        "            print('%s: %s' % (str(k), str(v)))\n",
        "\n",
        "    # Data processing helpers\n",
        "    def _get_width(self):\n",
        "        \"\"\" Returns image width \"\"\"\n",
        "        return 1920\n",
        "    def _get_height(self):\n",
        "        \"\"\" Returns image height \"\"\"\n",
        "        return 1080\n",
        "    def _get_dim(self):\n",
        "        \"\"\" Returns the image dimensions \"\"\"\n",
        "        return 1920, 1080\n",
        "\n",
        "    # Image processing helpers\n",
        "    def get_annotated_frame_numbers(self, set_id):\n",
        "        \"\"\" Generates and returns a dictionary of videos and annotated frames for each video in the give set\n",
        "        :param set_id: Set to generate annotated frames\n",
        "        :return: A dictionary of form : {<video_id>: [<number_of_frames>,<annotated_frame_id_0>,... <annotated_frame_id_n>]} \"\"\"\n",
        "        print(\"Generating annotated frame numbers for\", set_id)\n",
        "        annotated_frames_file = join(self._pie_path, \"annotations\", set_id, set_id + '_annotated_frames.csv')\n",
        "        # If the file exists, load from the file\n",
        "        if isfile(annotated_frames_file):\n",
        "            with open(annotated_frames_file, 'rt') as f:\n",
        "                annotated_frames = {x.split(',')[0]:\n",
        "                                        [int(fr) for fr in x.split(',')[1:]] for x in f.readlines()}\n",
        "            return annotated_frames\n",
        "        else:\n",
        "            # Generate annotated frame ids for each video\n",
        "            annotated_frames = {v.split('_annt.xml')[0]: [] for v in sorted(listdir(join(self._annotation_path,\n",
        "                                                                                         set_id))) if\n",
        "                                v.endswith(\"annt.xml\")}\n",
        "            for vid, annot_frames in sorted(annotated_frames.items()):\n",
        "                _frames = []\n",
        "                path_to_file = join(self._annotation_path, set_id, vid + '_annt.xml')\n",
        "                tree = ET.parse(path_to_file)\n",
        "                tracks = tree.findall('./track')\n",
        "                for t in tracks:\n",
        "                    if t.get('label') != 'pedestrian':\n",
        "                        continue\n",
        "                    boxes = t.findall('./box')\n",
        "                    for b in boxes:\n",
        "                        # Exclude the annotations that are outside of the frame\n",
        "                        if int(b.get('outside')) == 1:\n",
        "                            continue\n",
        "                        _frames.append(int(b.get('frame')))\n",
        "                _frames = sorted(list(set(_frames)))\n",
        "                annot_frames.append(len(_frames))\n",
        "                annot_frames.extend(_frames)\n",
        "\n",
        "            with open(annotated_frames_file, 'wt') as fid:\n",
        "                for vid, annot_frames in sorted(annotated_frames.items()):\n",
        "                    fid.write(vid)\n",
        "                    for fr in annot_frames:\n",
        "                        fid.write(\",\" + str(fr))\n",
        "                    fid.write('\\n')\n",
        "        return annotated_frames\n",
        "\n",
        "    def get_frame_numbers(self, set_id):\n",
        "        \"\"\" Generates and returns a dictionary of videos and  frames for each video in the give set\n",
        "        :param set_id: Set to generate annotated frames\n",
        "        :return: A dictionary of form: {<video_id>: [<number_of_frames>,<frame_id_0>,... <frame_id_n>]} \"\"\"\n",
        "        print(\"Generating frame numbers for\", set_id)\n",
        "        frame_ids = {v.split('_annt.xml')[0]: [] for v in sorted(listdir(join(self._annotation_path,\n",
        "                                                                              set_id))) if\n",
        "                     v.endswith(\"annt.xml\")}\n",
        "        for vid, frames in sorted(frame_ids.items()):\n",
        "            path_to_file = join(self._annotation_path, set_id, vid + '_annt.xml')\n",
        "            tree = ET.parse(path_to_file)\n",
        "            num_frames = int(tree.find(\"./meta/task/size\").text)\n",
        "            frames.extend([i for i in range(num_frames)])\n",
        "            frames.insert(0, num_frames)\n",
        "        return frame_ids\n",
        "\n",
        "    def extract_and_save_images(self, extract_frame_type='annotated'):\n",
        "        \"\"\" Extracts images from clips and saves on hard drive\n",
        "        :param extract_frame_type: Whether to extract 'all' frames or only the ones that are 'annotated' \"\"\"\n",
        "        set_folders = [f for f in sorted(listdir(self._clips_path))]\n",
        "        for set_id in set_folders:\n",
        "            print('Extracting frames from', set_id)\n",
        "            set_folder_path = join(self._clips_path, set_id)\n",
        "            if extract_frame_type == 'annotated':\n",
        "                extract_frames = self.get_annotated_frame_numbers(set_id)\n",
        "            else:\n",
        "                extract_frames = self.get_frame_numbers(set_id)\n",
        "            set_images_path = join(self._pie_path, \"images\", set_id)\n",
        "            for vid, frames in sorted(extract_frames.items()):\n",
        "                print(vid)\n",
        "                video_images_path = join(set_images_path, vid)\n",
        "                num_frames = frames[0]\n",
        "                frames_list = frames[1:]\n",
        "                if not isdir(video_images_path):\n",
        "                    makedirs(video_images_path)\n",
        "                vidcap = cv2.VideoCapture(join(set_folder_path, vid + '.mp4'))\n",
        "                success, image = vidcap.read()\n",
        "                frame_num = 0\n",
        "                img_count = 0\n",
        "                if not success:\n",
        "                    print('Failed to open the video {}'.format(vid))\n",
        "                while success:\n",
        "                    if frame_num in frames_list:\n",
        "                        self.update_progress(img_count / num_frames)\n",
        "                        img_count += 1\n",
        "                        if not isfile(join(video_images_path, \"%05.f.png\") % frame_num):\n",
        "                            cv2.imwrite(join(video_images_path, \"%05.f.png\") % frame_num, image)\n",
        "                    success, image = vidcap.read()\n",
        "                    frame_num += 1\n",
        "                if num_frames != img_count:\n",
        "                    print('num images don\\'t match {}/{}'.format(num_frames, img_count))\n",
        "                print('\\n')\n",
        "\n",
        "    # Annotation processing helpers\n",
        "    def _map_text_to_scalar(self, label_type, value):\n",
        "        \"\"\" Maps a text label in XML file to scalars\n",
        "        :param label_type: The label type\n",
        "        :param value: The text to be mapped\n",
        "        :return: The scalar value \"\"\"\n",
        "\n",
        "        map_dic = {'occlusion': {'none': 0, 'part': 1, 'full': 2},\n",
        "                   'action': {'standing': 0, 'walking': 1},\n",
        "                   'look': {'not-looking': 0, 'looking': 1},\n",
        "                   'gesture': {'__undefined__': 0, 'hand_ack': 1, 'hand_yield': 2,\n",
        "                               'hand_rightofway': 3, 'nod': 4, 'other': 5},\n",
        "                   'cross': {'not-crossing': 0, 'crossing': 1, 'crossing-irrelevant': -1},\n",
        "                   'crossing': {'not-crossing': 0, 'crossing': 1, 'irrelevant': -1},\n",
        "                   'age': {'child': 0, 'young': 1, 'adult': 2, 'senior': 3},\n",
        "                   'designated': {'ND': 0, 'D': 1},\n",
        "                   'gender': {'n/a': 0, 'female': 1, 'male': 2},\n",
        "                   'intersection': {'midblock': 0, 'T': 1, 'T-left': 2, 'T-right': 3, 'four-way': 4},\n",
        "                   'motion_direction': {'n/a': 0, 'LAT': 1, 'LONG': 2},\n",
        "                   'traffic_direction': {'OW': 0, 'TW': 1},\n",
        "                   'signalized': {'n/a': 0, 'C': 1, 'S': 2, 'CS': 3},\n",
        "                   'vehicle': {'car': 0, 'truck': 1, 'bus': 2, 'train': 3, 'bicycle': 4, 'bike': 5},\n",
        "                   'sign': {'ped_blue': 0, 'ped_yellow': 1, 'ped_white': 2, 'ped_text': 3, 'stop_sign': 4,\n",
        "                            'bus_stop': 5, 'train_stop': 6, 'construction': 7, 'other': 8},\n",
        "                   'traffic_light': {'regular': 0, 'transit': 1, 'pedestrian': 2},\n",
        "                   'state': {'__undefined__': 0, 'red': 1, 'yellow': 2, 'green': 3}}\n",
        "\n",
        "        return map_dic[label_type][value]\n",
        "\n",
        "    def _map_scalar_to_text(self, label_type, value):\n",
        "        \"\"\"    Maps a scalar value to a text label\n",
        "        :param label_type: The label type\n",
        "        :param value: The scalar to be mapped\n",
        "        :return: The text label   \"\"\"\n",
        "\n",
        "        map_dic = {'occlusion': {0: 'none', 1: 'part', 2: 'full'},\n",
        "                   'action': {0: 'standing', 1: 'walking'},\n",
        "                   'look': {0: 'not-looking', 1: 'looking'},\n",
        "                   'hand_gesture': {0: '__undefined__', 1: 'hand_ack',\n",
        "                                    2: 'hand_yield', 3: 'hand_rightofway',\n",
        "                                    4: 'nod', 5: 'other'},\n",
        "                   'cross': {0: 'not-crossing', 1: 'crossing', -1: 'crossing-irrelevant'},\n",
        "                   'crossing': {0: 'not-crossing', 1: 'crossing', -1: 'irrelevant'},\n",
        "                   'age': {0: 'child', 1: 'young', 2: 'adult', 3: 'senior'},\n",
        "                   'designated': {0: 'ND', 1: 'D'},\n",
        "                   'gender': {0: 'n/a', 1: 'female', 2: 'male'},\n",
        "                   'intersection': {0: 'midblock', 1: 'T', 2: 'T-left', 3: 'T-right', 4: 'four-way'},\n",
        "                   'motion_direction': {0: 'n/a', 1: 'LAT', 2: 'LONG'},\n",
        "                   'traffic_direction': {0: 'OW', 1: 'TW'},\n",
        "                   'signalized': {0: 'n/a', 1: 'C', 2: 'S', 3: 'CS'},\n",
        "                   'vehicle': {0: 'car', 1: 'truck', 2: 'bus', 3: 'train', 4: 'bicycle', 5: 'bike'},\n",
        "                   'sign': {0: 'ped_blue', 1: 'ped_yellow', 2: 'ped_white', 3: 'ped_text', 4: 'stop_sign',\n",
        "                            5: 'bus_stop', 6: 'train_stop', 7: 'construction', 8: 'other'},\n",
        "                   'traffic_light': {0: 'regular', 1: 'transit', 2: 'pedestrian'},\n",
        "                   'state': {0: '__undefined__', 1: 'red', 2: 'yellow', 3: 'green'}}\n",
        "\n",
        "        return map_dic[label_type][value]\n",
        "\n",
        "    def _get_annotations(self, setid, vid):\n",
        "        \"\"\" Generates a dictionary of annotations by parsing the video XML file\n",
        "        :param setid: The set id\n",
        "        :param vid: The video id\n",
        "        :return: A dictionary of annotations \"\"\"\n",
        "        path_to_file = join(self._annotation_path, setid, vid + '_annt.xml')\n",
        "        print(path_to_file)\n",
        "        tree = ET.parse(path_to_file)\n",
        "        ped_annt = 'ped_annotations'\n",
        "        traffic_annt = 'traffic_annotations'\n",
        "        annotations = {}\n",
        "        annotations['num_frames'] = int(tree.find(\"./meta/task/size\").text)\n",
        "        annotations['width'] = int(tree.find(\"./meta/task/original_size/width\").text)\n",
        "        annotations['height'] = int(tree.find(\"./meta/task/original_size/height\").text)\n",
        "        annotations[ped_annt] = {}\n",
        "        annotations[traffic_annt] = {}\n",
        "        tracks = tree.findall('./track')\n",
        "        for t in tracks:\n",
        "            boxes = t.findall('./box')\n",
        "            obj_label = t.get('label')\n",
        "            obj_id = boxes[0].find('./attribute[@name=\\\"id\\\"]').text\n",
        "            if obj_label == 'pedestrian':\n",
        "                annotations[ped_annt][obj_id] = {'frames': [], 'bbox': [], 'occlusion': []}\n",
        "                annotations[ped_annt][obj_id]['behavior'] = {'gesture': [], 'look': [], 'action': [], 'cross': []}\n",
        "                for b in boxes:\n",
        "                    # Exclude the annotations that are outside of the frame\n",
        "                    if int(b.get('outside')) == 1:\n",
        "                        continue\n",
        "                    annotations[ped_annt][obj_id]['bbox'].append(\n",
        "                        [float(b.get('xtl')), float(b.get('ytl')),\n",
        "                         float(b.get('xbr')), float(b.get('ybr'))])\n",
        "                    occ = self._map_text_to_scalar('occlusion', b.find('./attribute[@name=\\\"occlusion\\\"]').text)\n",
        "                    annotations[ped_annt][obj_id]['occlusion'].append(occ)\n",
        "                    annotations[ped_annt][obj_id]['frames'].append(int(b.get('frame')))\n",
        "                    for beh in annotations['ped_annotations'][obj_id]['behavior']:\n",
        "                        # Read behavior tags for each frame and add to the database\n",
        "                        annotations[ped_annt][obj_id]['behavior'][beh].append(\n",
        "                            self._map_text_to_scalar(beh, b.find('./attribute[@name=\\\"' + beh + '\\\"]').text))\n",
        "            else:\n",
        "                obj_type = boxes[0].find('./attribute[@name=\\\"type\\\"]')\n",
        "                if obj_type is not None:\n",
        "                    obj_type = self._map_text_to_scalar(obj_label, boxes[0].find('./attribute[@name=\\\"type\\\"]').text)\n",
        "                annotations[traffic_annt][obj_id] = {'frames': [], 'bbox': [], 'occlusion': [],\n",
        "                                                     'obj_class': obj_label,\n",
        "                                                     'obj_type': obj_type,\n",
        "                                                     'state': []}\n",
        "                for b in boxes:\n",
        "                    # Exclude the annotations that are outside of the frame\n",
        "                    if int(b.get('outside')) == 1:\n",
        "                        continue\n",
        "                    annotations[traffic_annt][obj_id]['bbox'].append(\n",
        "                        [float(b.get('xtl')), float(b.get('ytl')),\n",
        "                         float(b.get('xbr')), float(b.get('ybr'))])\n",
        "                    annotations[traffic_annt][obj_id]['occlusion'].append(int(b.get('occluded')))\n",
        "                    annotations[traffic_annt][obj_id]['frames'].append(int(b.get('frame')))\n",
        "                    if obj_label == 'traffic_light':\n",
        "                        annotations[traffic_annt][obj_id]['state'].append(self._map_text_to_scalar('state', b.find('./attribute[@name=\\\"state\\\"]').text))\n",
        "        return annotations\n",
        "\n",
        "    def _get_ped_attributes(self, setid, vid):\n",
        "        \"\"\"  Generates a dictionary of attributes by parsing the video XML file\n",
        "        :param setid: The set id\n",
        "        :param vid: The video id\n",
        "        :return: A dictionary of attributes  \"\"\"\n",
        "        path_to_file = join(self._annotation_attributes_path, setid, vid + '_attributes.xml')\n",
        "        tree = ET.parse(path_to_file)\n",
        "        attributes = {}\n",
        "        pedestrians = tree.findall(\"./pedestrian\")\n",
        "        for p in pedestrians:\n",
        "            ped_id = p.get('id')\n",
        "            attributes[ped_id] = {}\n",
        "            for k, v in p.items():\n",
        "                if 'id' in k:\n",
        "                    continue\n",
        "                try:\n",
        "                    if k == 'intention_prob':\n",
        "                        attributes[ped_id][k] = float(v)\n",
        "                    else:\n",
        "                        attributes[ped_id][k] = int(v)\n",
        "                except ValueError:\n",
        "                    attributes[ped_id][k] = self._map_text_to_scalar(k, v)\n",
        "        return attributes\n",
        "\n",
        "    def _get_vehicle_attributes(self, setid, vid):\n",
        "        \"\"\"  Generates a dictionary of vehicle attributes by parsing the video XML file\n",
        "        :param setid: The set id\n",
        "        :param vid: The video id\n",
        "        :return: A dictionary of vehicle attributes (obd sensor recording)   \"\"\"\n",
        "        path_to_file = join(self._annotation_vehicle_path, setid, vid + '_obd.xml')\n",
        "        tree = ET.parse(path_to_file)\n",
        "        veh_attributes = {}\n",
        "        frames = tree.findall(\"./frame\")\n",
        "        for f in frames:\n",
        "            dict_vals = {k: float(v) for k, v in f.attrib.items() if k != 'id'}\n",
        "            veh_attributes[int(f.get('id'))] = dict_vals\n",
        "        return veh_attributes\n",
        "\n",
        "    def generate_database(self):\n",
        "        \"\"\"    Generates and saves a database of the pie dataset by integrating all annotations\n",
        "        Dictionary structure:\n",
        "        'set_id'(str): {\n",
        "            'vid_id'(str): {\n",
        "                'num_frames': int\n",
        "                'width': int\n",
        "                'height': int\n",
        "                'traffic_annotations'(str): {\n",
        "                    'obj_id'(str): {\n",
        "                        'frames': list(int)\n",
        "                        'occlusion': list(int)\n",
        "                        'bbox': list([x1, y1, x2, y2]) (float)\n",
        "                        'obj_class': str,\n",
        "                        'obj_type': str,    # only for traffic lights, vehicles, signs\n",
        "                        'state': list(int)  # only for traffic lights\n",
        "                'ped_annotations'(str): {\n",
        "                    'ped_id'(str): {\n",
        "                        'frames': list(int)\n",
        "                        'occlusion': list(int)\n",
        "                        'bbox': list([x1, y1, x2, y2]) (float)\n",
        "                        'behavior'(str): {\n",
        "                            'action': list(int)\n",
        "                            'gesture': list(int)\n",
        "                            'cross': list(int)\n",
        "                            'look': list(int)\n",
        "                        'attributes'(str): {\n",
        "                             'age': int\n",
        "                             'id': str\n",
        "                             'num_lanes': int\n",
        "                             'crossing': int\n",
        "                             'gender': int\n",
        "                             'crossing_point': int\n",
        "                             'critical_point': int\n",
        "                             'exp_start_point': int\n",
        "                             'intersection': int\n",
        "                             'designated': int\n",
        "                             'signalized': int\n",
        "                             'traffic_direction': int\n",
        "                             'group_size': int\n",
        "                             'motion_direction': int\n",
        "                'vehicle_annotations'(str){\n",
        "                    'frame_id'(int){'longitude': float\n",
        "                          'yaw': float\n",
        "                          'pitch': float\n",
        "                          'roll': float\n",
        "                          'OBD_speed': float\n",
        "                          'GPS_speed': float\n",
        "                          'latitude': float\n",
        "                          'longitude': float\n",
        "                          'heading_angle': float\n",
        "                          'accX': float\n",
        "                          'accY': float\n",
        "                          'accZ: float\n",
        "                          'gyroX': float\n",
        "                          'gyroY': float\n",
        "                          'gyroZ': float\n",
        "\n",
        "        :return: A database dictionary  \"\"\"\n",
        "        print('---------------------------------------------------------')\n",
        "        print(\"Generating database for pie\")\n",
        "        cache_file = join(self.cache_path, 'pie_database.pkl')\n",
        "        if isfile(cache_file) and not self._regen_database:\n",
        "            with open(cache_file, 'rb') as fid:\n",
        "                try:\n",
        "                    database = pickle.load(fid)\n",
        "                except:\n",
        "                    database = pickle.load(fid, encoding='bytes')\n",
        "            print('pie annotations loaded from {}'.format(cache_file))\n",
        "            return database\n",
        "        # Path to the folder annotations\n",
        "        set_ids = [f for f in sorted(listdir(self._annotation_path))]\n",
        "        # Read the content of set folders\n",
        "        database = {}\n",
        "        for setid in set_ids:\n",
        "            video_ids = [v.split('_annt.xml')[0] for v in sorted(listdir(join(self._annotation_path,\n",
        "                                                                              setid))) if v.endswith(\"annt.xml\")]\n",
        "            database[setid] = {}\n",
        "            for vid in video_ids:\n",
        "                print('Getting annotations for %s, %s' % (setid, vid))\n",
        "                database[setid][vid] = self._get_annotations(setid, vid)\n",
        "                vid_attributes = self._get_ped_attributes(setid, vid)\n",
        "                database[setid][vid]['vehicle_annotations'] = self._get_vehicle_attributes(setid, vid)\n",
        "                for ped in database[setid][vid]['ped_annotations']:\n",
        "                    database[setid][vid]['ped_annotations'][ped]['attributes'] = vid_attributes[ped]\n",
        "        with open(cache_file, 'wb') as fid:\n",
        "            pickle.dump(database, fid, pickle.HIGHEST_PROTOCOL)\n",
        "        print('The database is written to {}'.format(cache_file))\n",
        "        return database\n",
        "\n",
        "    def get_data_stats(self):\n",
        "        \"\"\" Generates statistics for the dataset \"\"\"\n",
        "        annotations = self.generate_database()\n",
        "        set_count = len(annotations.keys())\n",
        "        ped_count = 0\n",
        "        ped_box_count = 0\n",
        "        video_count = 0\n",
        "        total_frames = 0\n",
        "        age = {'child': 0, 'adult': 0, 'senior': 0}\n",
        "        gender = {'male': 0, 'female': 0}\n",
        "        signalized = {'n/a': 0, 'C': 0, 'S': 0, 'CS': 0}\n",
        "        traffic_direction = {'OW': 0, 'TW': 0}\n",
        "        intersection = {'midblock': 0, 'T': 0, 'T-right': 0, 'T-left': 0, 'four-way': 0}\n",
        "        crossing = {'crossing': 0, 'not-crossing': 0, 'irrelevant': 0}\n",
        "        traffic_obj_types = {'vehicle': {'car': 0, 'truck': 0, 'bus': 0, 'train': 0, 'bicycle': 0, 'bike': 0},\n",
        "                             'sign': {'ped_blue': 0, 'ped_yellow': 0, 'ped_white': 0, 'ped_text': 0, 'stop_sign': 0,\n",
        "                                      'bus_stop': 0, 'train_stop': 0, 'construction': 0, 'other': 0},\n",
        "                             'traffic_light': {'regular': 0, 'transit': 0, 'pedestrian': 0},\n",
        "                             'crosswalk': 0,\n",
        "                             'transit_station': 0}\n",
        "        traffic_box_count = {'vehicle': 0, 'traffic_light': 0, 'sign': 0, 'crosswalk': 0, 'transit_station': 0}\n",
        "        for sid, vids in annotations.items():\n",
        "            video_count += len(vids)\n",
        "            for vid, annots in vids.items():\n",
        "                total_frames += annots['num_frames']\n",
        "                for trf_ids, trf_annots in annots['traffic_annotations'].items():\n",
        "                    obj_class = trf_annots['obj_class']\n",
        "                    traffic_box_count[obj_class] += len(trf_annots['frames'])\n",
        "                    if obj_class in ['traffic_light', 'vehicle', 'sign']:\n",
        "                        obj_type = trf_annots['obj_type']\n",
        "                        traffic_obj_types[obj_class][self._map_scalar_to_text(obj_class, obj_type)] += 1\n",
        "                    else:\n",
        "                        traffic_obj_types[obj_class] += 1\n",
        "                for ped_ids, ped_annots in annots['ped_annotations'].items():\n",
        "                    ped_count += 1\n",
        "                    ped_box_count += len(ped_annots['frames'])\n",
        "                    age[self._map_scalar_to_text('age', ped_annots['attributes']['age'])] += 1\n",
        "                    if self._map_scalar_to_text('crossing', ped_annots['attributes']['crossing']) == 'crossing':\n",
        "                        crossing[self._map_scalar_to_text('crossing', ped_annots['attributes']['crossing'])] += 1\n",
        "                    else:\n",
        "                        if ped_annots['attributes']['intention_prob'] > 0.5:\n",
        "                            crossing['not-crossing'] += 1\n",
        "                        else:\n",
        "                            crossing['irrelevant'] += 1\n",
        "                    intersection[\n",
        "                        self._map_scalar_to_text('intersection', ped_annots['attributes']['intersection'])] += 1\n",
        "                    traffic_direction[self._map_scalar_to_text('traffic_direction',\n",
        "                                                               ped_annots['attributes']['traffic_direction'])] += 1\n",
        "                    signalized[self._map_scalar_to_text('signalized', ped_annots['attributes']['signalized'])] += 1\n",
        "                    gender[self._map_scalar_to_text('gender', ped_annots['attributes']['gender'])] += 1\n",
        "\n",
        "        print('---------------------------------------------------------')\n",
        "        print(\"Number of sets: %d\" % set_count)\n",
        "        print(\"Number of videos: %d\" % video_count)\n",
        "        print(\"Number of annotated frames: %d\" % total_frames)\n",
        "        print(\"Number of pedestrians %d\" % ped_count)\n",
        "        print(\"age:\\n\", '\\n '.join('{}: {}'.format(tag, cnt) for tag, cnt in sorted(age.items())))\n",
        "        print(\"gender:\\n\", '\\n '.join('{}: {}'.format(tag, cnt) for tag, cnt in sorted(gender.items())))\n",
        "        print(\"signal:\\n\", '\\n '.join('{}: {}'.format(tag, cnt) for tag, cnt in sorted(signalized.items())))\n",
        "        print(\"traffic direction:\\n\",\n",
        "              '\\n '.join('{}: {}'.format(tag, cnt) for tag, cnt in sorted(traffic_direction.items())))\n",
        "        print(\"crossing:\\n\", '\\n '.join('{}: {}'.format(tag, cnt) for tag, cnt in sorted(crossing.items())))\n",
        "        print(\"intersection:\\n\", '\\n '.join('{}: {}'.format(tag, cnt) for tag, cnt in sorted(intersection.items())))\n",
        "        print(\"Number of pedestrian bounding boxes: %d\" % ped_box_count)\n",
        "        print(\"Number of traffic objects\")\n",
        "        for trf_obj, values in sorted(traffic_obj_types.items()):\n",
        "            if isinstance(values, dict):\n",
        "                print(trf_obj + ':\\n', '\\n '.join('{}: {}'.format(k, v) for k, v in sorted(values.items())),\n",
        "                      '\\n total: ', sum(values.values()))\n",
        "            else:\n",
        "                print(trf_obj + ': %d' % values)\n",
        "        print(\"Number of pedestrian bounding boxes:\\n\",\n",
        "              '\\n '.join('{}: {}'.format(tag, cnt) for tag, cnt in sorted(traffic_box_count.items())),\n",
        "              '\\n total: ', sum(traffic_box_count.values()))\n",
        "\n",
        "    def balance_samples_count(self, seq_data, label_type, random_seed=1234):\n",
        "        \"\"\"   Balances the number of positive and negative samples by randomly sampling\n",
        "        from the more represented samples. Only works for binary classes.\n",
        "        :param seq_data: The sequence data to be balanced.\n",
        "        :param label_type: The lable type based on which the balancing takes place.\n",
        "        The label values must be binary, i.e. only 0, 1.\n",
        "        :param random_seed: The seed for random number generator.\n",
        "        :return: Balanced data sequence.   \"\"\"\n",
        "        for lbl in seq_data[label_type]:\n",
        "            for i in lbl:\n",
        "                if i[0] not in [0, 1]:\n",
        "                    raise Exception(\"The label values used for balancing must be either 0 or 1\")\n",
        "        # balances the number of positive and negative samples\n",
        "        print('---------------------------------------------------------')\n",
        "        print(\"Balancing the number of positive and negative intention samples\")\n",
        "        gt_labels = [gt[0] for gt in seq_data[label_type]]\n",
        "        num_pos_samples = np.count_nonzero(np.array(gt_labels))\n",
        "        num_neg_samples = len(gt_labels) - num_pos_samples\n",
        "        new_seq_data = {}\n",
        "        # finds the indices of the samples with larger quantity\n",
        "        if num_neg_samples == num_pos_samples:\n",
        "            print('Positive and negative samples are already balanced')\n",
        "            return seq_data\n",
        "        else:\n",
        "            print('Unbalanced: \\t Positive: {} \\t Negative: {}'.format(num_pos_samples, num_neg_samples))\n",
        "            if num_neg_samples > num_pos_samples:\n",
        "                rm_index = np.where(np.array(gt_labels) == 0)[0]\n",
        "            else:\n",
        "                rm_index = np.where(np.array(gt_labels) == 1)[0]\n",
        "\n",
        "            # Calculate the difference of sample counts\n",
        "            dif_samples = abs(num_neg_samples - num_pos_samples)\n",
        "            # shuffle the indices\n",
        "            np.random.seed(random_seed)\n",
        "            np.random.shuffle(rm_index)\n",
        "            # reduce the number of indices to the difference\n",
        "            rm_index = rm_index[0:dif_samples]\n",
        "            # update the data\n",
        "            for k in seq_data:\n",
        "                seq_data_k = seq_data[k]\n",
        "                if not isinstance(seq_data[k], list):\n",
        "                    new_seq_data[k] = seq_data[k]\n",
        "                else:\n",
        "                    new_seq_data[k] = [seq_data_k[i] for i in range(0, len(seq_data_k)) if i not in rm_index]\n",
        "\n",
        "            new_gt_labels = [gt[0] for gt in new_seq_data[label_type]]\n",
        "            num_pos_samples = np.count_nonzero(np.array(new_gt_labels))\n",
        "            print('Balanced:\\t Positive: %d  \\t Negative: %d\\n'\n",
        "                  % (num_pos_samples, len(new_seq_data[label_type]) - num_pos_samples))\n",
        "        return new_seq_data\n",
        "\n",
        "    # Process pedestrian ids\n",
        "    def _get_pedestrian_ids(self):\n",
        "        \"\"\" Returns a list with all pedestrian ids \"\"\"\n",
        "        annotations = self.generate_database()\n",
        "        pids = []\n",
        "        for sid in sorted(annotations):\n",
        "            for vid in sorted(annotations[sid]):\n",
        "                pids.extend(annotations[sid][vid]['ped_annotations'].keys())\n",
        "        return pids\n",
        "\n",
        "    # Trajectory data generation\n",
        "    def _get_data_ids(self, image_set, params):\n",
        "        \"\"\" Generates set ids and ped ids (if needed) for processing\n",
        "              :param image_set: Image-set to generate data\n",
        "              :param params: Data generation params\n",
        "              :return: Set and pedestrian ids    \"\"\"\n",
        "        _pids = None\n",
        "        if params['data_split_type'] == 'default':\n",
        "            set_ids = self._get_image_set_ids(image_set)\n",
        "        else:\n",
        "            set_ids = self._get_image_set_ids('all')\n",
        "        if params['data_split_type'] == 'random':\n",
        "            _pids = self._get_random_pedestrian_ids(image_set, **params['random_params'])\n",
        "        elif params['data_split_type'] == 'kfold':\n",
        "            _pids = self._get_kfold_pedestrian_ids(image_set, **params['kfold_params'])\n",
        "        return set_ids, _pids\n",
        "\n",
        "    def _squarify(self, bbox, ratio, img_width):\n",
        "        \"\"\" Changes the ratio of bounding boxes to a fixed ratio\n",
        "        :param bbox: Bounding box\n",
        "        :param ratio: Ratio to be changed to\n",
        "        :param img_width: Image width\n",
        "        :return: Squarified boduning box \"\"\"\n",
        "        width = abs(bbox[0] - bbox[2])\n",
        "        height = abs(bbox[1] - bbox[3])\n",
        "        width_change = height * ratio - width\n",
        "        bbox[0] = bbox[0] - width_change / 2\n",
        "        bbox[2] = bbox[2] + width_change / 2\n",
        "        if bbox[0] < 0:\n",
        "            bbox[0] = 0\n",
        "\n",
        "        # check whether the new bounding box goes beyond image boarders\n",
        "        # If this is the case, the bounding box is shifted back\n",
        "        if bbox[2] > img_width:\n",
        "            bbox[0] = bbox[0] - bbox[2] + img_width\n",
        "            bbox[2] = img_width\n",
        "        return bbox\n",
        "\n",
        "    def _height_check(self, height_rng, frame_ids, boxes, images, occlusion):\n",
        "        \"\"\" Checks whether the bounding boxes are within a given height limit. If not, it\n",
        "        will adjust the length of bounding boxes in data sequences accordingly\n",
        "        :param height_rng: Height limit [lower, higher]\n",
        "        :param frame_ids: List of frame ids\n",
        "        :param boxes: List of bounding boxes\n",
        "        :param images: List of images\n",
        "        :param occlusion: List of occlusions\n",
        "        :return: The adjusted data sequences \"\"\"\n",
        "        imgs, box, frames, occ = [], [], [], []\n",
        "        for i, b in enumerate(boxes):\n",
        "            bbox_height = abs(b[1] - b[3])\n",
        "            if height_rng[0] <= bbox_height <= height_rng[1]:\n",
        "                box.append(b)\n",
        "                imgs.append(images[i])\n",
        "                frames.append(frame_ids[i])\n",
        "                occ.append(occlusion[i])\n",
        "        return imgs, box, frames, occ\n",
        "\n",
        "    def _get_center(self, box):\n",
        "        \"\"\" Calculates the center coordinate of a bounding box\n",
        "        :param box: Bounding box coordinates\n",
        "        :return: The center coordinate \"\"\"\n",
        "        return [(box[0] + box[2]) / 2, (box[1] + box[3]) / 2]\n",
        "\n",
        "    def generate_data_trajectory_sequence(self, image_set, **opts):\n",
        "        \"\"\" Generates pedestrian tracks\n",
        "        :param image_set: the split set to produce for. Options are train, test, val.\n",
        "        :param opts:\n",
        "        'fstride': Frequency of sampling from the data.\n",
        "        'height_rng': The height range of pedestrians to use.\n",
        "        'squarify_ratio': The width/height ratio of bounding boxes. A value between (0,1]. 0 the original ratio is used.\n",
        "        'data_split_type': How to split the data. Options: 'default', predefined sets, 'random', randomly split the data, and 'kfold', k-fold data split (NOTE: only train/test splits)\n",
        "        'seq_type': Sequence type to generate. Options: 'trajectory', generates tracks, 'crossing', generates tracks up to 'crossing_point', 'intention' generates tracks similar to human experiments\n",
        "        'min_track_size': Min track length allowable.\n",
        "        :return: Sequence data \"\"\"\n",
        "\n",
        "        params = {'fstride': 1,\n",
        "                  'sample_type': 'all',\n",
        "                  'height_rng': [0, float('inf')],\n",
        "                  'squarify_ratio': 0,\n",
        "                  'data_split_type': 'default',  # kfold, random, default\n",
        "                  'seq_type': 'intention',\n",
        "                  'min_track_size': 3}\n",
        "\n",
        "        for i in opts.keys():\n",
        "            params[i] = opts[i]\n",
        "\n",
        "        print('---------------------------------------------------------')\n",
        "        print(\"Generating trajectory sequence data\")\n",
        "        self._print_dict(params)\n",
        "        annot_database = self.generate_database()\n",
        "        if params['seq_type'] == 'trajectory':\n",
        "            sequence_data = self._get_trajectories(image_set, annot_database, **params)\n",
        "        elif params['seq_type'] == 'crossing':\n",
        "            sequence_data = self._get_crossing(image_set, annot_database, **params)\n",
        "        elif params['seq_type'] == 'intention':\n",
        "            sequence_data = self._get_intention(image_set, annot_database, **params)\n",
        "\n",
        "        return sequence_data\n",
        "\n",
        "    def _get_trajectories(self, image_set, annotations, **params):\n",
        "        \"\"\"  Generates trajectory data.\n",
        "        :param image_set: Data split to use\n",
        "        :param annotations: Annotations database\n",
        "        :param params: Parameters to generate data\n",
        "        :return: A dictionary of trajectories \"\"\"\n",
        "        print('---------------------------------------------------------')\n",
        "        print(\"Generating trajectory data\")\n",
        "        num_pedestrians = 0\n",
        "        seq_stride = params['fstride']\n",
        "        sq_ratio = params['squarify_ratio']\n",
        "        height_rng = params['height_rng']\n",
        "        image_seq, pids_seq = [], []\n",
        "        box_seq, center_seq, occ_seq = [], [], []\n",
        "        intent_seq = []\n",
        "        obds_seq, gpss_seq, head_ang_seq, gpsc_seq, yrp_seq = [], [], [], [], []\n",
        "        set_ids, _pids = self._get_data_ids(image_set, params)\n",
        "        for sid in set_ids:\n",
        "            for vid in sorted(annotations[sid]):\n",
        "                img_width = annotations[sid][vid]['width']\n",
        "                pid_annots = annotations[sid][vid]['ped_annotations']\n",
        "                vid_annots = annotations[sid][vid]['vehicle_annotations']\n",
        "                for pid in sorted(pid_annots):\n",
        "                    if params['data_split_type'] != 'default' and pid not in _pids:\n",
        "                        continue\n",
        "                    num_pedestrians += 1\n",
        "                    frame_ids = pid_annots[pid]['frames']\n",
        "                    boxes = pid_annots[pid]['bbox']\n",
        "                    images = [self._get_image_path(sid, vid, f) for f in frame_ids]\n",
        "                    occlusions = pid_annots[pid]['occlusion']\n",
        "                    if height_rng[0] > 0 or height_rng[1] < float('inf'):\n",
        "                        images, boxes, frame_ids, occlusions = self._height_check(height_rng, frame_ids, boxes, images, occlusions)\n",
        "                    if len(boxes) / seq_stride < params['min_track_size']:\n",
        "                        continue\n",
        "                    if sq_ratio:\n",
        "                        boxes = [self._squarify(b, sq_ratio, img_width) for b in boxes]\n",
        "                    image_seq.append(images[::seq_stride])\n",
        "                    box_seq.append(boxes[::seq_stride])\n",
        "                    center_seq.append([self._get_center(b) for b in boxes][::seq_stride])\n",
        "                    occ_seq.append(occlusions[::seq_stride])\n",
        "                    ped_ids = [[pid]] * len(boxes)\n",
        "                    pids_seq.append(ped_ids[::seq_stride])\n",
        "                    intent = [[pid_annots[pid]['attributes']['intention_prob']]] * len(boxes)\n",
        "                    intent_seq.append(intent[::seq_stride])\n",
        "                    gpsc_seq.append([(vid_annots[i]['latitude'], vid_annots[i]['longitude'])\n",
        "                                     for i in frame_ids][::seq_stride])\n",
        "                    obds_seq.append([[vid_annots[i]['OBD_speed']] for i in frame_ids][::seq_stride])\n",
        "                    gpss_seq.append([[vid_annots[i]['GPS_speed']] for i in frame_ids][::seq_stride])\n",
        "                    head_ang_seq.append([[vid_annots[i]['heading_angle']] for i in frame_ids][::seq_stride])\n",
        "                    yrp_seq.append([(vid_annots[i]['yaw'], vid_annots[i]['roll'], vid_annots[i]['pitch'])\n",
        "                                    for i in frame_ids][::seq_stride])\n",
        "\n",
        "        print('Subset: %s' % image_set)\n",
        "        print('Number of pedestrians: %d ' % num_pedestrians)\n",
        "        print('Total number of samples: %d ' % len(image_seq))\n",
        "        return {'image': image_seq,\n",
        "                'pid': pids_seq,\n",
        "                'bbox': box_seq,\n",
        "                'center': center_seq,\n",
        "                'occlusion': occ_seq,\n",
        "                'obd_speed': obds_seq,\n",
        "                'gps_speed': gpss_seq,\n",
        "                'heading_angle': head_ang_seq,\n",
        "                'gps_coord': gpsc_seq,\n",
        "                'yrp': yrp_seq,\n",
        "                'intention_prob': intent_seq}\n",
        "\n",
        "    def _get_intention(self, image_set, annotations, **params):\n",
        "        \"\"\"   Generates intention data.\n",
        "                  :param image_set: Data split to use\n",
        "                  :param annotations: Annotations database\n",
        "                  :param params: Parameters to generate data (see generade_database)\n",
        "                  :return: A dictionary of trajectories  \"\"\"\n",
        "        print('---------------------------------------------------------')\n",
        "        print(\"Generating intention data\")\n",
        "        num_pedestrians = 0\n",
        "        seq_stride = params['fstride']\n",
        "        sq_ratio = params['squarify_ratio']\n",
        "        height_rng = params['height_rng']\n",
        "        intention_prob, intention_binary = [], []\n",
        "        image_seq, pids_seq = [], []\n",
        "        box_seq, center_seq, occ_seq = [], [], []\n",
        "        set_ids, _pids = self._get_data_ids(image_set, params)\n",
        "        for sid in set_ids:\n",
        "            for vid in sorted(annotations[sid]):\n",
        "                img_width = annotations[sid][vid]['width']\n",
        "                pid_annots = annotations[sid][vid]['ped_annotations']\n",
        "                for pid in sorted(pid_annots):\n",
        "                    if params['data_split_type'] != 'default' and pid not in _pids:\n",
        "                        continue\n",
        "                    num_pedestrians += 1\n",
        "                    exp_start_frame = pid_annots[pid]['attributes']['exp_start_point']\n",
        "                    critical_frame = pid_annots[pid]['attributes']['critical_point']\n",
        "                    frames = pid_annots[pid]['frames']\n",
        "                    start_idx = frames.index(exp_start_frame)\n",
        "                    end_idx = frames.index(critical_frame)\n",
        "                    boxes = pid_annots[pid]['bbox'][start_idx:end_idx + 1]\n",
        "                    frame_ids = frames[start_idx:end_idx + 1]\n",
        "                    images = [self._get_image_path(sid, vid, f) for f in frame_ids]\n",
        "                    occlusions = pid_annots[pid]['occlusion'][start_idx:end_idx + 1]\n",
        "                    if height_rng[0] > 0 or height_rng[1] < float('inf'):\n",
        "                        images, boxes, frame_ids, occlusions = self._height_check(height_rng,\n",
        "                                                                                  frame_ids, boxes,\n",
        "                                                                                  images, occlusions)\n",
        "                    if len(boxes) / seq_stride < params['min_track_size']:\n",
        "                        continue\n",
        "                    if sq_ratio:\n",
        "                        boxes = [self._squarify(b, sq_ratio, img_width) for b in boxes]\n",
        "                    int_prob = [[pid_annots[pid]['attributes']['intention_prob']]] * len(boxes)\n",
        "                    int_bin = [[int(pid_annots[pid]['attributes']['intention_prob'] > 0.5)]] * len(boxes)\n",
        "                    image_seq.append(images[::seq_stride])\n",
        "                    box_seq.append(boxes[::seq_stride])\n",
        "                    occ_seq.append(occlusions[::seq_stride])\n",
        "                    intention_prob.append(int_prob[::seq_stride])\n",
        "                    intention_binary.append(int_bin[::seq_stride])\n",
        "                    ped_ids = [[pid]] * len(boxes)\n",
        "                    pids_seq.append(ped_ids[::seq_stride])\n",
        "\n",
        "        print('Subset: %s' % image_set)\n",
        "        print('Number of pedestrians: %d ' % num_pedestrians)\n",
        "        print('Total number of samples: %d ' % len(image_seq))\n",
        "\n",
        "        return {'image': image_seq,\n",
        "                'bbox': box_seq,\n",
        "                'occlusion': occ_seq,\n",
        "                'intention_prob': intention_prob,\n",
        "                'intention_binary': intention_binary,\n",
        "                'ped_id': pids_seq}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "ZXI4ss2sFM2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace2d36a-2cfc-4012-d465-8a56723b6b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45192\n"
          ]
        }
      ],
      "source": [
        "# @title extract frames from clips\n",
        "\n",
        "#pie_path = '/content/drive/My Drive/Colab Notebooks/CV/PIE_data'\n",
        "#imdb = PIE(data_path=pie_path)\n",
        "#imdb.extract_and_save_images(extract_frame_type='annotated')\n",
        "\n",
        "def count_files_in_folder(directory):\n",
        "    total = 0\n",
        "    for path, dirs, files in os.walk(directory):\n",
        "        total += len(files)\n",
        "    return total\n",
        "img= '/content/drive/My Drive/Colab Notebooks/CV/PIE_data/images'\n",
        "imgs = count_files_in_folder(img)\n",
        "print(imgs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "4zmTEueiA-YP"
      },
      "outputs": [],
      "source": [
        "# @title generate annotations_database\n",
        "\n",
        "#pie_path = '/content/drive/My Drive/Colab Notebooks/CV/PIE_data'\n",
        "#data = PIE(data_path=pie_path)\n",
        "#data.generate_database()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WO5Ru79tc_Fw",
        "outputId": "1a22f678-e3a3-42e7-d3fa-460050b477da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------\n",
            "Generating trajectory sequence data\n",
            "fstride: 1\n",
            "sample_type: all\n",
            "height_rng: [0, inf]\n",
            "squarify_ratio: 0\n",
            "data_split_type: default\n",
            "seq_type: intention\n",
            "min_track_size: 3\n",
            "---------------------------------------------------------\n",
            "Generating database for pie\n",
            "pie annotations loaded from /content/drive/My Drive/Colab Notebooks/CV/PIE_data/data_cache/pie_database.pkl\n",
            "---------------------------------------------------------\n",
            "Generating intention data\n",
            "Subset: train\n",
            "Number of pedestrians: 111 \n",
            "Total number of samples: 111 \n",
            "---------------------------------------------------------\n",
            "Generating trajectory sequence data\n",
            "fstride: 1\n",
            "sample_type: all\n",
            "height_rng: [0, inf]\n",
            "squarify_ratio: 0\n",
            "data_split_type: default\n",
            "seq_type: intention\n",
            "min_track_size: 3\n",
            "---------------------------------------------------------\n",
            "Generating database for pie\n",
            "pie annotations loaded from /content/drive/My Drive/Colab Notebooks/CV/PIE_data/data_cache/pie_database.pkl\n",
            "---------------------------------------------------------\n",
            "Generating intention data\n",
            "Subset: val\n",
            "Number of pedestrians: 80 \n",
            "Total number of samples: 80 \n",
            "---------------------------------------------------------\n",
            "Generating trajectory sequence data\n",
            "fstride: 1\n",
            "sample_type: all\n",
            "height_rng: [0, inf]\n",
            "squarify_ratio: 0\n",
            "data_split_type: default\n",
            "seq_type: intention\n",
            "min_track_size: 3\n",
            "---------------------------------------------------------\n",
            "Generating database for pie\n",
            "pie annotations loaded from /content/drive/My Drive/Colab Notebooks/CV/PIE_data/data_cache/pie_database.pkl\n",
            "---------------------------------------------------------\n",
            "Generating intention data\n",
            "Subset: test\n",
            "Number of pedestrians: 76 \n",
            "Total number of samples: 76 \n"
          ]
        }
      ],
      "source": [
        "# @title generate data_splits\n",
        "pie_path = '/content/drive/My Drive/Colab Notebooks/CV/PIE_data'\n",
        "data = PIE( data_path = pie_path )\n",
        "train = data.generate_data_trajectory_sequence('train', data_split_type = 'default')\n",
        "val = data.generate_data_trajectory_sequence('val', data_split_type = 'default')\n",
        "test = data.generate_data_trajectory_sequence('test', data_split_type = 'default')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB2b5gLZrT2P",
        "outputId": "bc9fda0e-201e-499d-8330-6fcbdb7accff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['image', 'bbox', 'occlusion', 'intention_prob', 'intention_binary', 'ped_id'])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "print(len(train))\n",
        "train.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovrfJlekwxgZ"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "cellView": "form",
        "id": "qmlZ7Avpi5_f"
      },
      "outputs": [],
      "source": [
        "# @title utilities\n",
        "\n",
        "def update_progress(progress):\n",
        "    barLength = 20\n",
        "    status = \"\"\n",
        "    if isinstance(progress, int):\n",
        "        progress = float(progress)\n",
        "    block = int(round(barLength*progress))\n",
        "    text = \"\\r[{}] {:0.2f}% {}\".format( \"#\"*block + \"-\"*(barLength-block), progress*100, status)\n",
        "    sys.stdout.write(text)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def img_pad(img, mode = 'warp', size = 224):\n",
        "    ''' Pads a given image.\n",
        "    Crops and/or pads a image given the boundries of the box needed\n",
        "    img: the image to be coropped and/or padded\n",
        "    bbox: the bounding box dimensions for cropping\n",
        "    size: the desired size of output\n",
        "    mode: the type of padding or resizing. The modes are,\n",
        "        warp: crops the bounding box and resize to the output size\n",
        "        same: only crops the image\n",
        "        pad_same: maintains the original size of the cropped box  and pads with zeros\n",
        "        pad_resize: crops the image and resize the cropped box in a way that the longer edge is equal to\n",
        "        the desired output size in that direction while maintaining the aspect ratio. The rest of the image is\n",
        "        padded with zeros\n",
        "        pad_fit: maintains the original size of the cropped box unless the image is biger than the size in which case\n",
        "        it scales the image down, and then pads it  '''\n",
        "\n",
        "    assert(mode in ['same', 'warp', 'pad_same', 'pad_resize', 'pad_fit']), 'Pad mode %s is invalid' % mode\n",
        "    image = img.copy()\n",
        "    if mode == 'warp':\n",
        "        warped_image = image.resize((size,size),PIL.Image.NEAREST)\n",
        "        return warped_image\n",
        "    elif mode == 'same':\n",
        "        return image\n",
        "    elif mode in ['pad_same','pad_resize','pad_fit']:\n",
        "        img_size = image.size  # size is in (width, height)\n",
        "        ratio = float(size)/max(img_size)\n",
        "        if mode == 'pad_resize' or  \\\n",
        "            (mode == 'pad_fit' and (img_size[0] > size or img_size[1] > size)):\n",
        "            img_size = tuple([int(img_size[0]*ratio),int(img_size[1]*ratio)])\n",
        "            image = image.resize(img_size, PIL.Image.NEAREST)\n",
        "        padded_image = PIL.Image.new(\"RGB\", (size, size))\n",
        "        padded_image.paste(image, ((size-img_size [0])//2,\n",
        "                    (size-img_size [1])//2))\n",
        "        return padded_image\n",
        "\n",
        "def squarify(bbox, squarify_ratio, img_width):\n",
        "    width = abs(bbox[0] - bbox[2])\n",
        "    height = abs(bbox[1] - bbox[3])\n",
        "    width_change = height * squarify_ratio - width\n",
        "    bbox[0] = bbox[0] - width_change/2\n",
        "    bbox[2] = bbox[2] + width_change/2\n",
        "    if bbox[0] < 0:\n",
        "        bbox[0] = 0\n",
        "    # check whether the new bounding box goes beyond image boarders\n",
        "    # If this is the case, the bounding box is shifted back\n",
        "    if bbox[2] > img_width:\n",
        "        bbox[0] = bbox[0]-bbox[2] + img_width\n",
        "        bbox[2] = img_width\n",
        "    return bbox\n",
        "\n",
        "def bbox_sanity_check(img, bbox):\n",
        "    ''' This is to confirm that the bounding boxes are within image boundaries.\n",
        "    If this is not the case, modifications is applied.\n",
        "    This is to deal with inconsistencies in the annotation tools  '''\n",
        "    img_width, img_heigth = img.size\n",
        "    if bbox[0] < 0:\n",
        "        bbox[0] = 0.0\n",
        "    if bbox[1] < 0:\n",
        "        bbox[1] = 0.0\n",
        "    if bbox[2] >= img_width:\n",
        "        bbox[2] = img_width - 1\n",
        "    if bbox[3] >= img_heigth:\n",
        "        bbox[3] = img_heigth - 1\n",
        "    return bbox\n",
        "\n",
        "def jitter_bbox(img_path,bbox, mode, ratio):\n",
        "    ''' This method jitters the position or dimentions of the bounding box.\n",
        "    mode: 'same' returns the bounding box unchanged\n",
        "          'enlarge' increases the size of bounding box based on the given ratio.\n",
        "          'random_enlarge' increases the size of bounding box by randomly sampling a value in [0,ratio)\n",
        "          'move' moves the center of the bounding box in each direction based on the given ratio\n",
        "          'random_move' moves the center of the bounding box in each direction by randomly sampling a value in [-ratio,ratio)\n",
        "    ratio: The ratio of change relative to the size of the bounding box. For modes 'enlarge' and 'random_enlarge'\n",
        "           the absolute value is considered.\n",
        "    Note: Tha ratio of change in pixels is calculated according to the smaller dimension of the bounding box '''\n",
        "    assert(mode in ['same','enlarge','move','random_enlarge','random_move']), \\\n",
        "            'mode %s is invalid.' % mode\n",
        "    if mode == 'same':\n",
        "        return bbox\n",
        "    img = load_img(img_path)\n",
        "    img_width, img_heigth = img.size\n",
        "    if mode in ['random_enlarge', 'enlarge']:\n",
        "        jitter_ratio  = abs(ratio)\n",
        "    else:\n",
        "        jitter_ratio  = ratio\n",
        "    if mode == 'random_enlarge':\n",
        "        jitter_ratio = np.random.random_sample()*jitter_ratio\n",
        "    elif mode == 'random_move':\n",
        "        # for ratio between (-jitter_ratio, jitter_ratio)\n",
        "        # for sampling the formula is [a,b), b > a,\n",
        "        # random_sample * (b-a) + a\n",
        "        jitter_ratio = np.random.random_sample() * jitter_ratio * 2 - jitter_ratio\n",
        "    jit_boxes = []\n",
        "    for b in bbox:\n",
        "        bbox_width = b[2] - b[0]\n",
        "        bbox_height = b[3] - b[1]\n",
        "        width_change = bbox_width * jitter_ratio\n",
        "        height_change = bbox_height * jitter_ratio\n",
        "        if width_change < height_change:\n",
        "            height_change = width_change\n",
        "        else:\n",
        "            width_change = height_change\n",
        "        if mode in ['enlarge','random_enlarge']:\n",
        "            b[0] = b[0] - width_change //2\n",
        "            b[1] = b[1] - height_change //2\n",
        "        else:\n",
        "            b[0] = b[0] + width_change //2\n",
        "            b[1] = b[1] + height_change //2\n",
        "        b[2] = b[2] + width_change //2\n",
        "        b[3] = b[3] + height_change //2\n",
        "        # Checks to make sure the bbox is not exiting the image boundaries\n",
        "        b =  bbox_sanity_check(img, b)\n",
        "        jit_boxes.append(b)\n",
        "    return jit_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "flw2ATHszmj3"
      },
      "outputs": [],
      "source": [
        "# @title Model\n",
        "\"\"\"The code implementation of the paper:\n",
        "A. Rasouli, I. Kotseruba, T. Kunic, and J. Tsotsos, \"PIE: A Large-Scale Dataset and Models for Pedestrian Intention Estimation and Trajectory Prediction\", ICCV 2019.\"\"\"\n",
        "\n",
        "class PIEIntent(object):\n",
        "    \"\"\" A convLSTM encoder decoder model for predicting pedestrian intention\n",
        "         Attributes:\n",
        "        _num_hidden_units: Number of LSTM hidden units\n",
        "        _reg_value: the value of L2 regularizer for training\n",
        "        _kernel_regularizer: Training regularizer set as L2\n",
        "        _recurrent_regularizer: Training regularizer set as L2\n",
        "        _activation: LSTM activations\n",
        "        _lstm_dropout: input dropout\n",
        "        _lstm_recurrent_dropout: recurrent dropout\n",
        "        _convlstm_num_filters: number of filters in convLSTM\n",
        "        _convlstm_kernel_size: kernel size in convLSTM\n",
        "    Model attributes: set during training depending on the data\n",
        "        _encoder_input_size: size of the encoder input\n",
        "        _decoder_input_size: size of the encoder_output\n",
        "    Methods:\n",
        "        load_images_and_process: generates trajectories by sampling from pedestrian sequences\n",
        "        get_data_slices: generate tracks for training/testing\n",
        "        pie_convlstm_encdec: generates intention prediction model\n",
        "        train: trains the model\n",
        "        test_chunk: tests the model (chunks the test samples for memory efficiency) \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_hidden_units=128,\n",
        "                 regularizer_val=0.001,\n",
        "                 activation='tanh',\n",
        "                 lstm_dropout=0.4,\n",
        "                 lstm_recurrent_dropout=0.2,\n",
        "                 convlstm_num_filters=64,\n",
        "                 convlstm_kernel_size=2):\n",
        "\n",
        "        # Network parameters\n",
        "        self._num_hidden_units = num_hidden_units\n",
        "        self.reg_value = regularizer_val\n",
        "        self._kernel_regularizer = regularizers.l2(regularizer_val)\n",
        "        self._recurrent_regularizer = regularizers.l2(regularizer_val)\n",
        "        self._bias_regularizer = regularizers.l2(regularizer_val)\n",
        "        self._activation = activation\n",
        "        # Encoder\n",
        "        self._lstm_dropout = lstm_dropout\n",
        "        self._lstm_recurrent_dropout = lstm_recurrent_dropout\n",
        "        # conv unit parameters\n",
        "        self._convlstm_num_filters = convlstm_num_filters\n",
        "        self._convlstm_kernel_size = convlstm_kernel_size\n",
        "        self._encoder_input_size = 4  # decided on run time according to data\n",
        "        self._decoder_dense_output_size = 1\n",
        "        self._decoder_input_size = 4  # decided on run time according to data\n",
        "        self._model_name = 'convlstm_encdec'\n",
        "\n",
        "    def get_path(self,\n",
        "                 type_save='models', # model or data\n",
        "                 models_save_folder='',\n",
        "                 model_name='convlstm_encdec',\n",
        "                 file_name='',\n",
        "                 data_subset='',\n",
        "                 data_type='',\n",
        "                 save_root_folder='/content/drive/My Drive/Colab Notebooks/CV/PIE_data' + '/data/'):\n",
        "        \"\"\"  A path generator method for saving model and config data. Creates directories as needed.\n",
        "        :param type_save: Specifies whether data or model is saved.\n",
        "        :param models_save_folder: model name (e.g. train function uses timestring \"%d%b%Y-%Hh%Mm%Ss\")\n",
        "        :param model_name: model name (either trained convlstm_encdec model or vgg16)\n",
        "        :param file_name: Actual name of the file\n",
        "        :param data_subset: train, test or val\n",
        "        :param data_type: type of the data (e.g. features_context_pad_resize)\n",
        "        :param save_root_folder: The root folder for saved data.\n",
        "        :return: The full path for the save folder \"\"\"\n",
        "        assert(type_save in ['models', 'data'])\n",
        "        if data_type != '':\n",
        "            assert(any([d in data_type for d in ['images', 'features']]))\n",
        "        root = os.path.join(save_root_folder, type_save)\n",
        "        if type_save == 'models':\n",
        "            save_path = os.path.join(save_root_folder, 'pie', 'intention', models_save_folder)\n",
        "            if not os.path.exists(save_path):\n",
        "                os.makedirs(save_path)\n",
        "            return os.path.join(save_path, file_name), save_path\n",
        "        else:\n",
        "            save_path = os.path.join(root, 'pie', data_subset, data_type, model_name)\n",
        "            if not os.path.exists(save_path):\n",
        "                os.makedirs(save_path)\n",
        "            return save_path\n",
        "\n",
        "    def get_model_config(self):\n",
        "        \"\"\" Returns a dictionary containing model configuration \"\"\"\n",
        "        config = dict()\n",
        "        # Network parameters\n",
        "        config['num_hidden'] = self._num_hidden_units\n",
        "        config['reg_value'] = self.reg_value\n",
        "        config['activation'] = self._activation\n",
        "        config['sequence_length'] = self._sequence_length\n",
        "        config['lstm_dropout'] = self._lstm_dropout\n",
        "        config['lstm_recurrent_dropout'] = self._lstm_recurrent_dropout\n",
        "        config['convlstm_num_filters'] = self._convlstm_num_filters\n",
        "        config['convlstm_kernel_size'] = self._convlstm_kernel_size\n",
        "        config['encoder_input_size'] = self._encoder_input_size\n",
        "        config['decoder_input_size'] = self._decoder_input_size\n",
        "        config['decoder_dense_output_size'] = self._decoder_dense_output_size\n",
        "        config['encoder_seq_length'] = self._encoder_seq_length\n",
        "        config['decoder_seq_length'] = self._decoder_seq_length\n",
        "        print(config)\n",
        "        return config\n",
        "\n",
        "    def load_model_config(self, config):\n",
        "        \"\"\" Copy config information from the dictionary for testing \"\"\"\n",
        "        # Network parameters\n",
        "        self._num_hidden_units = config['num_hidden']\n",
        "        self.reg_value = config['reg_value']\n",
        "        self._activation = config['activation']\n",
        "        self._encoder_input_size = config['encoder_input_size']\n",
        "        self._encoder_seq_length = config['encoder_seq_length']\n",
        "        self._sequence_length = config['sequence_length']\n",
        "        self._lstm_dropout = config['lstm_dropout']\n",
        "        self._lstm_recurrent_dropout = config['lstm_recurrent_dropout']\n",
        "        self._convlstm_num_filters = config['convlstm_num_filters']\n",
        "        self._convlstm_kernel_size = config['convlstm_kernel_size']\n",
        "        self._encoder_input_size = config['decoder_input_size']\n",
        "        self._decoder_input_size = config['decoder_input_size']\n",
        "        self._decoder_dense_output_size = config['decoder_dense_output_size']\n",
        "        self._decoder_seq_length = config['decoder_seq_length']\n",
        "\n",
        "    def load_images_and_process(self,\n",
        "                                img_sequences,\n",
        "                                bbox_sequences,\n",
        "                                ped_ids,\n",
        "                                save_path,\n",
        "                                data_type='',\n",
        "                                regen_pkl=False):\n",
        "        \"\"\" Generates image features for convLSTM input. The images are first\n",
        "        cropped to 1.5x the size of the bounding box, padded and resized to\n",
        "        (224, 224) and fed into pretrained VGG16.\n",
        "        :param img_sequences: a list of frame names\n",
        "        :param bbox_sequences: a list of corresponding bounding boxes\n",
        "        :ped_ids: a list of pedestrian ids associated with the sequences\n",
        "        :save_path: path to save the precomputed features\n",
        "        :data_type: train/val/test data set\n",
        "        :regen_pkl: if set to True overwrites previously saved features\n",
        "        :return: a list of image features \"\"\"\n",
        "        # load the feature files, if exists\n",
        "        print(\"Generating {} features crop_type=context crop_mode=pad_resize \\nsave_path={}, \".format(data_type, save_path))\n",
        "        try:\n",
        "            convnet = self.context_model\n",
        "        except:\n",
        "            raise Exception(\"No context model is defined\")\n",
        "        sequences = []\n",
        "        i = -1\n",
        "        for seq, pid in zip(img_sequences, ped_ids):\n",
        "            i += 1\n",
        "            update_progress(i / len(img_sequences))\n",
        "            img_seq = []\n",
        "            for imp, b, p in zip(seq, bbox_sequences[i], pid):\n",
        "                set_id = imp.split('/')[-3]\n",
        "                vid_id = imp.split('/')[-2]\n",
        "                img_name = imp.split('/')[-1].split('.')[0]\n",
        "                img_save_folder = os.path.join(save_path, set_id, vid_id)\n",
        "                img_save_path = os.path.join(img_save_folder, img_name+'_'+p[0]+'.pkl')\n",
        "                if os.path.exists(img_save_path) and not regen_pkl:\n",
        "                    with open(img_save_path, 'rb') as fid:\n",
        "                        try:\n",
        "                            img_features = pickle.load(fid)\n",
        "                        except:\n",
        "                            img_features = pickle.load(fid, encoding='bytes')\n",
        "                else:\n",
        "                    img_data = load_img(imp)\n",
        "                    bbox = jitter_bbox(imp, [b],'enlarge', 2)[0]\n",
        "                    bbox = squarify(bbox, 1, img_data.size[0])\n",
        "                    bbox = list(map(int,bbox[0:4]))\n",
        "                    cropped_image = img_data.crop(bbox)\n",
        "                    img_data = img_pad(cropped_image, mode='pad_resize', size=224)\n",
        "                    image_array = img_to_array(img_data)\n",
        "                    preprocessed_img = vgg16.preprocess_input(image_array)\n",
        "                    expanded_img = np.expand_dims(preprocessed_img, axis=0)\n",
        "                    img_features = convnet.predict(expanded_img)\n",
        "                    if not os.path.exists(img_save_folder):\n",
        "                        os.makedirs(img_save_folder)\n",
        "                    with open(img_save_path, 'wb') as fid:\n",
        "                        pickle.dump(img_features, fid, pickle.HIGHEST_PROTOCOL)\n",
        "                img_features = np.squeeze(img_features)\n",
        "                img_seq.append(img_features)\n",
        "            sequences.append(img_seq)\n",
        "        sequences = np.array(sequences)\n",
        "        return sequences\n",
        "\n",
        "    def get_tracks(self, dataset, data_type, seq_length, overlap):\n",
        "        \"\"\" Generate tracks by sampling from pedestrian sequences\n",
        "        :param dataset: raw data from the dataset\n",
        "        :param data_type: types of data for encoder/decoder input\n",
        "        :param seq_length: the length of the sequence\n",
        "        :param overlap: defines the overlap between consecutive sequences (between 0 and 1)\n",
        "        :return: a dictionary containing sampled tracks for each data modality \"\"\"\n",
        "        overlap_stride = seq_length if overlap == 0 else \\\n",
        "        int((1 - overlap) * seq_length)\n",
        "        overlap_stride = 1 if overlap_stride < 1 else overlap_stride\n",
        "        d_types = []\n",
        "        for k in data_type.keys():\n",
        "            d_types.extend(data_type[k])\n",
        "        d = {}\n",
        "        if 'bbox' in d_types:\n",
        "            d['bbox'] = dataset['bbox']\n",
        "        if 'intention_binary' in d_types:\n",
        "            d['intention_binary'] = dataset['intention_binary']\n",
        "        if 'intention_prob' in d_types:\n",
        "            d['intention_prob'] = dataset['intention_prob']\n",
        "        bboxes = dataset['bbox'].copy()\n",
        "        images = dataset['image'].copy()\n",
        "        ped_ids = dataset['ped_id'].copy()\n",
        "        for k in d.keys():\n",
        "            tracks = []\n",
        "            for track in d[k]:\n",
        "                tracks.extend([track[i:i+seq_length] for i in\\\n",
        "                             range(0,len(track)\\\n",
        "                            - seq_length + 1, overlap_stride)])\n",
        "            d[k] = tracks\n",
        "        pid = []\n",
        "        for p in ped_ids:\n",
        "            pid.extend([p[i:i+seq_length] for i in\\\n",
        "                         range(0,len(p)\\\n",
        "                        - seq_length + 1, overlap_stride)])\n",
        "        ped_ids = pid\n",
        "        im = []\n",
        "        for img in images:\n",
        "            im.extend([img[i:i+seq_length] for i in\\\n",
        "                         range(0,len(img)\\\n",
        "                        - seq_length + 1, overlap_stride)])\n",
        "        images = im\n",
        "        bb = []\n",
        "        for bbox in bboxes:\n",
        "            bb.extend([bbox[i:i+seq_length] for i in\\\n",
        "                         range(0,len(bbox)\\\n",
        "                        - seq_length + 1, overlap_stride)])\n",
        "        bboxes = bb\n",
        "        return d, images, bboxes, ped_ids\n",
        "\n",
        "    def concat_data(self, data, data_type):\n",
        "        \"\"\"  Concatenates different types of data specified by data_type.\n",
        "        Creats dummy data if no data type is specified\n",
        "        :param data_type: type of data (e.g. bbox)  \"\"\"\n",
        "        if not data_type:\n",
        "            return []\n",
        "        # if more than one data type is specified, they are concatenated\n",
        "        d = []\n",
        "        for dt in data_type:\n",
        "            d.append(np.array(data[dt]))\n",
        "        if len(d) > 1:\n",
        "            d = np.concatenate(d, axis=2)\n",
        "        else:\n",
        "            d = d[0]\n",
        "        return d\n",
        "\n",
        "    def get_train_val_data(self, data, data_type, seq_length, overlap):\n",
        "        \"\"\" A helper function for data generation that combines different data types into a single\n",
        "        representation.\n",
        "        :param data: A dictionary of data types\n",
        "        :param data_type: The data types defined for encoder and decoder\n",
        "        :return: A unified data representation as a list  \"\"\"\n",
        "        tracks, images, bboxes, ped_ids = self.get_tracks(data, data_type, seq_length, overlap)\n",
        "        # Generate observation data input to encoder\n",
        "        encoder_input = self.concat_data(tracks, data_type['encoder_input_type'])\n",
        "        decoder_input = self.concat_data(tracks, data_type['decoder_input_type'])\n",
        "        output = self.concat_data(tracks, data_type['output_type'])\n",
        "        if len(decoder_input) == 0:\n",
        "            decoder_input = np.zeros(shape=np.array(bboxes).shape)\n",
        "        # Create context model\n",
        "        self.context_model = vgg16.VGG16(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "        return {'images': images,\n",
        "                'bboxes': bboxes,\n",
        "                'ped_ids': ped_ids,\n",
        "                'encoder_input': encoder_input,\n",
        "                'decoder_input': decoder_input,\n",
        "                'output': output}\n",
        "\n",
        "    def get_test_data(self, data, train_params, seq_length, overlap):\n",
        "        \"\"\" A helper function for test data generation that preprocesses the images, combines\n",
        "        different representations required as inputs to encoder and decoder, as well as\n",
        "        ground truth and returns them as a unified list representation.\n",
        "        :param data: A dictionary of data types\n",
        "        :param train_params: Training parameters defining the type of\n",
        "        :param data_type: The data types defined for encoder and decoder\n",
        "        :return: A unified data representation as a list   \"\"\"\n",
        "        tracks, images, bboxes, ped_ids = self.get_tracks(data, train_params['data_type'], seq_length, overlap)\n",
        "        # Generate observation data input to encoder\n",
        "        encoder_input = self.concat_data(tracks, train_params['data_type']['encoder_input_type'])\n",
        "        decoder_input = self.concat_data(tracks, train_params['data_type']['decoder_input_type'])\n",
        "        output = self.concat_data(tracks, train_params['data_type']['output_type'])\n",
        "        if len(decoder_input) == 0:\n",
        "            decoder_input = np.zeros(shape=np.array(bboxes).shape)\n",
        "        # Create context model\n",
        "        self.context_model = vgg16.VGG16(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "        test_img = self.load_images_and_process(images,\n",
        "                                                bboxes,\n",
        "                                                ped_ids,\n",
        "                                                data_type='test',\n",
        "                                                save_path=self.get_path(type_save='data',\n",
        "                                                                        data_type='features_context_pad_resize',  # images\n",
        "                                                                        model_name='vgg16_none',\n",
        "                                                                        data_subset='test'))\n",
        "        output = output[:, 0]\n",
        "        return ([test_img, decoder_input], output)\n",
        "\n",
        "    def get_model(self, model):\n",
        "        train_model = self.pie_convlstm_encdec()\n",
        "        return train_model\n",
        "\n",
        "    def pie_convlstm_encdec(self):\n",
        "        '''  Create an LSTM Encoder-Decoder model for intention estimation  '''\n",
        "        #Generate input data. the shapes is (sequence_lenght, length of flattened features)\n",
        "        encoder_input=input_data=Input(shape=(self._sequence_length,) + self.context_model.output_shape[1:], name = \"encoder_input\")\n",
        "        interm_input = encoder_input\n",
        "        # Generate Encoder LSTM Unit\n",
        "        encoder_model = ConvLSTM2D(filters=self._convlstm_num_filters,\n",
        "                                   kernel_size=self._convlstm_kernel_size,\n",
        "                                   kernel_regularizer=self._kernel_regularizer,\n",
        "                                   recurrent_regularizer=self._recurrent_regularizer,\n",
        "                                   bias_regularizer=self._bias_regularizer,\n",
        "                                   dropout=self._lstm_dropout,\n",
        "                                   recurrent_dropout=self._lstm_recurrent_dropout,\n",
        "                                   return_sequences=False)(interm_input)\n",
        "        encoder_output = Flatten(name='encoder_flatten')(encoder_model)\n",
        "        # Generate Decoder LSTM unit\n",
        "        decoder_input = Input(shape=(self._decoder_seq_length,\n",
        "                                     self._decoder_input_size),\n",
        "                              name='decoder_input')\n",
        "        encoder_vec = RepeatVector(self._decoder_seq_length)(encoder_output)\n",
        "        decoder_concat_inputs = Concatenate(axis=2)([encoder_vec, decoder_input])\n",
        "        decoder_model= LSTM(units=self._num_hidden_units,\n",
        "                             dropout=self._lstm_dropout,\n",
        "                             recurrent_dropout=self._lstm_recurrent_dropout,\n",
        "                             return_state= False,\n",
        "                             return_sequences=False,\n",
        "                             stateful=False,\n",
        "                             bias_initializer='zeros',\n",
        "                             kernel_regularizer=self._kernel_regularizer,\n",
        "                             recurrent_regularizer=self._recurrent_regularizer,\n",
        "                             bias_regularizer=self._bias_regularizer,\n",
        "                             activation=self._activation,\n",
        "                             name='decoder_network')(decoder_concat_inputs)\n",
        "        decoder_dense_output = Dense(self._decoder_dense_output_size,\n",
        "                                     activation='sigmoid',\n",
        "                                     name='decoder_dense')(decoder_model)\n",
        "        decoder_output = decoder_dense_output\n",
        "        self.train_model = Model(inputs=[encoder_input, decoder_input], outputs=decoder_output)\n",
        "        self.train_model.summary()\n",
        "        return self.train_model\n",
        "\n",
        "    def train(self,\n",
        "              data_train,\n",
        "              data_val,\n",
        "              batch_size=32,\n",
        "              epochs=100,\n",
        "              optimizer_type='rmsprop',\n",
        "              optimizer_params={'learning_rate': 0.001, 'decay': 0.0},\n",
        "              loss=['binary_crossentropy'],\n",
        "              metrics=['acc'],\n",
        "              data_opts=''):\n",
        "        \"\"\" Training method for the model\n",
        "        :param data_train: training data\n",
        "        :param data_val: validation data\n",
        "        :param batch_size: batch size for training\n",
        "        :param epochs: number of epochs for training\n",
        "        :param optimizer_params: learning rate and decay\n",
        "        :param loss: type of loss function\n",
        "        :param metrics: metrics to monitor\n",
        "        :param data_opts: data generation parameters \"\"\"\n",
        "\n",
        "        data_type = {'encoder_input_type': data_opts['encoder_input_type'],\n",
        "                     'decoder_input_type': data_opts['decoder_input_type'],\n",
        "                     'output_type': data_opts['output_type']}\n",
        "\n",
        "        train_config = {'batch_size': batch_size,\n",
        "                        'epoch': epochs,\n",
        "                        'optimizer_type': optimizer_type,\n",
        "                        'optimizer_params': optimizer_params,\n",
        "                        'loss': loss,\n",
        "                        'metrics': metrics,\n",
        "                        'learning_scheduler_mode': 'plateau',\n",
        "                        'learning_scheduler_params': {'exp_decay_param': 0.3,\n",
        "                                                      'step_drop_rate': 0.5,\n",
        "                                                      'epochs_drop_rate': 20.0,\n",
        "                                                      'plateau_patience': 5,\n",
        "                                                      'min_lr': 0.00001,\n",
        "                                                      'monitor_value': 'val_loss'},\n",
        "                        'model': 'convlstm_encdec',\n",
        "                        'data_type': data_type,\n",
        "                        'overlap': data_opts['seq_overlap_rate'],\n",
        "                        'dataset': 'pie'}\n",
        "        self._model_type = 'convlstm_encdec'\n",
        "        seq_length = data_opts['max_size_observe']\n",
        "        train_d = self.get_train_val_data(data_train, data_type, seq_length, data_opts['seq_overlap_rate'])\n",
        "        val_d = self.get_train_val_data(data_val, data_type, seq_length, data_opts['seq_overlap_rate'])\n",
        "        self._encoder_seq_length = train_d['decoder_input'].shape[1]\n",
        "        self._decoder_seq_length = train_d['decoder_input'].shape[1]\n",
        "        self._sequence_length = self._encoder_seq_length\n",
        "        # Create context model\n",
        "        self.context_model = vgg16.VGG16(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "        train_img = self.load_images_and_process(train_d['images'],\n",
        "                                                 train_d['bboxes'],\n",
        "                                                 train_d['ped_ids'],\n",
        "                                                 data_type='train',\n",
        "                                                 save_path=self.get_path(type_save='data',\n",
        "                                                                         data_type='features'+'_'+data_opts['crop_type']+'_'+data_opts['crop_mode'], # images\n",
        "                                                                         model_name='vgg16_'+'none',\n",
        "                                                                         data_subset = 'train'))\n",
        "        val_img = self.load_images_and_process(val_d['images'],\n",
        "                                               val_d['bboxes'],\n",
        "                                               val_d['ped_ids'],\n",
        "                                               data_type='val',\n",
        "                                               save_path=self.get_path(type_save='data',\n",
        "                                                                       data_type='features'+'_'+data_opts['crop_type']+'_'+data_opts['crop_mode'],\n",
        "                                                                       model_name='vgg16_'+'none',\n",
        "                                                                       data_subset='val'))\n",
        "\n",
        "        train_model = self.pie_convlstm_encdec()\n",
        "        train_d['output'] = train_d['output'][:, 0]\n",
        "        val_d['output'] = val_d['output'][:, 0]\n",
        "        train_data = ([train_img, train_d['decoder_input']], train_d['output'])\n",
        "        val_data = ([val_img, val_d['decoder_input']], val_d['output'])\n",
        "        optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate=optimizer_params['learning_rate'], decay=optimizer_params['decay'])\n",
        "        train_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "        print('TRAINING: loss={} metrics={}'.format(loss, metrics))\n",
        "        model_folder_name = time.strftime(\"%d%b%Y-%Hh%Mm%Ss\")\n",
        "        model_path, _ = self.get_path(type_save='models',\n",
        "                                      model_name='convlstm_encdec',\n",
        "                                      models_save_folder=model_folder_name,\n",
        "                                      file_name='model.keras',\n",
        "                                      save_root_folder='data')\n",
        "        config_path, _ = self.get_path(type_save='models',\n",
        "                                       model_name='convlstm_encdec',\n",
        "                                       models_save_folder=model_folder_name,\n",
        "                                       file_name='configs',\n",
        "                                       save_root_folder='data')\n",
        "        #Save config and training param files\n",
        "        with open(config_path+'.pkl', 'wb') as fid:\n",
        "            pickle.dump([self.get_model_config(),\n",
        "                        train_config, data_opts],\n",
        "                        fid, pickle.HIGHEST_PROTOCOL)\n",
        "        print('Wrote configs to {}'.format(config_path))\n",
        "        with open(config_path+'.txt', 'wt') as fid:\n",
        "            fid.write(\"####### Data options #######\\n\")\n",
        "            fid.write(str(data_opts))\n",
        "            fid.write(\"\\n####### Model config #######\\n\")\n",
        "            fid.write(str(self.get_model_config()))\n",
        "            fid.write(\"\\n####### Training config #######\\n\")\n",
        "            fid.write(str(train_config))\n",
        "\n",
        "        early_stop = EarlyStopping(monitor='val_loss',\n",
        "                                   min_delta=0.0001,\n",
        "                                   patience=5,\n",
        "                                   verbose=1)\n",
        "        checkpoint = ModelCheckpoint(filepath=model_path,\n",
        "                                     save_best_only=True,\n",
        "                                     save_weights_only=False,\n",
        "                                     monitor=train_config['learning_scheduler_params']['monitor_value'])  #, mode = 'min'\n",
        "        plateau_sch = ReduceLROnPlateau(monitor=train_config['learning_scheduler_params']['monitor_value'],\n",
        "                factor=train_config['learning_scheduler_params']['step_drop_rate'],\n",
        "                patience=train_config['learning_scheduler_params']['plateau_patience'],\n",
        "                min_lr=train_config['learning_scheduler_params']['min_lr'],\n",
        "                verbose = 1)\n",
        "\n",
        "        call_backs = [checkpoint, early_stop, plateau_sch]\n",
        "        history = train_model.fit(x=train_data[0],\n",
        "                                  y=train_data[1],\n",
        "                                  batch_size=batch_size,\n",
        "                                  epochs=epochs,\n",
        "                                  validation_data=val_data,\n",
        "                                  callbacks=call_backs,\n",
        "                                  verbose=1)\n",
        "        history_path, saved_files_path = self.get_path(type_save='models',\n",
        "                                                       model_name='convlstm_encdec',\n",
        "                                                       models_save_folder=model_folder_name,\n",
        "                                                       file_name='history.pkl',\n",
        "                                                       save_root_folder='data')\n",
        "        with open(history_path, 'wb') as fid:\n",
        "            pickle.dump(history.history, fid, pickle.HIGHEST_PROTOCOL)\n",
        "        print('Wrote configs to {}'.format(config_path))\n",
        "\n",
        "        del train_data, val_data  # clear memory\n",
        "        del train_d, val_d\n",
        "        return saved_files_path\n",
        "\n",
        "    #split test data into chunks\n",
        "    def test_chunk(self,\n",
        "                   data_test,\n",
        "                   data_opts='',\n",
        "                   model_path='',\n",
        "                   visualize=False):\n",
        "            with open(os.path.join(model_path, 'configs.pkl'), 'rb') as fid:\n",
        "                try:\n",
        "                    configs = pickle.load(fid)\n",
        "                except:\n",
        "                    configs = pickle.load(fid, encoding='bytes')\n",
        "            train_params = configs[1]\n",
        "            self.load_model_config(configs[0])\n",
        "             # Create context model\n",
        "            self.context_model = vgg16.VGG16(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "            try:\n",
        "                test_model = load_model(os.path.join(model_path, 'model.keras'))\n",
        "            except:\n",
        "                test_model = self.get_model(train_params['model'])\n",
        "                test_model.load_weights(os.path.join(model_path, 'model.keras'))\n",
        "            test_model.summary()\n",
        "            overlap = 1  # train_params ['overlap']\n",
        "            test_target_data = []\n",
        "            test_results = []\n",
        "            ped_ids = []\n",
        "            images = []\n",
        "            bboxes = []\n",
        "            num_samples = len(data_test['image'])\n",
        "            vis_results = []\n",
        "\n",
        "            for i in range(0, len(data_test['image']), 19):\n",
        "                data_test_chunk = {}\n",
        "                data_test_chunk['intention_binary'] = data_test['intention_binary'][i:min(i+100, num_samples)]\n",
        "                data_test_chunk['image'] = data_test['image'][i:min(i+100,num_samples)]\n",
        "                data_test_chunk['ped_id'] = data_test['ped_id'][i:min(i+100,num_samples)]\n",
        "                data_test_chunk['intention_prob'] = data_test['intention_prob'][i:min(i+100,num_samples)]\n",
        "                data_test_chunk['bbox'] = data_test['bbox'][i:min(i+100,num_samples)]\n",
        "                test_data_chunk, test_target_data_chunk = self.get_test_data(data_test_chunk,\n",
        "                                                                                    train_params,\n",
        "                                                                                    self._sequence_length,\n",
        "                                                                                    overlap)\n",
        "                tracks, images_chunk, bboxes_chunk, ped_ids_chunk = self.get_tracks(data_test_chunk,\n",
        "                                                                                           train_params['data_type'],\n",
        "                                                                                           self._sequence_length,\n",
        "                                                                                           overlap)\n",
        "                test_results_chunk = test_model.predict(test_data_chunk,\n",
        "                                                        batch_size=train_params['batch_size'],\n",
        "                                                        verbose=1)\n",
        "                test_target_data.extend(test_target_data_chunk)\n",
        "                test_results.extend(test_results_chunk)\n",
        "                images.extend(images_chunk)\n",
        "                ped_ids.extend(ped_ids_chunk)\n",
        "                bboxes.extend(bboxes_chunk)\n",
        "                i = -1\n",
        "                for imp, box, ped in zip(images_chunk, bboxes_chunk, ped_ids_chunk):\n",
        "                    i+=1\n",
        "                    vis_results.append({'imp': imp[-1],\n",
        "                                        'bbox': box[-1],\n",
        "                                        'ped_id': ped[-1][0],\n",
        "                                        'res': test_results_chunk[i][0],\n",
        "                                        'target': test_target_data_chunk[i]})\n",
        "\n",
        "            acc = accuracy_score(test_target_data, np.round(test_results))\n",
        "            f1 = f1_score(test_target_data, np.round(test_results))\n",
        "            cm = confusion_matrix(test_target_data, np.round(test_results), normalize = 'all')\n",
        "            plt.figure(figsize=(10,7))\n",
        "            sns.heatmap(cm, annot=True)\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('Truth')\n",
        "            plt.show()\n",
        "            save_results_path = os.path.join(model_path, 'ped_intents.pkl')\n",
        "            if not os.path.exists(save_results_path):\n",
        "                results = {'ped_id': ped_ids,\n",
        "                           'images': images,\n",
        "                           'results': test_results,\n",
        "                           'gt': test_target_data}\n",
        "                with open(save_results_path, 'wb') as fid:\n",
        "                    pickle.dump(results, fid, pickle.HIGHEST_PROTOCOL)\n",
        "            return acc, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iPPQo2sK2Tnb",
        "outputId": "afd1211b-ce81-4e90-9d08-d3348cd63b49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating train features crop_type=context crop_mode=pad_resize \n",
            "save_path=/content/drive/My Drive/Colab Notebooks/CV/PIE_data/data/data/pie/train/features_context_pad_resize/vgg16_none, \n",
            "[####################] 99.92% Generating val features crop_type=context crop_mode=pad_resize \n",
            "save_path=/content/drive/My Drive/Colab Notebooks/CV/PIE_data/data/data/pie/val/features_context_pad_resize/vgg16_none, \n",
            "[####################] 99.89% Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_input (InputLayer)  [(None, 15, 7, 7, 512)]      0         []                            \n",
            "                                                                                                  \n",
            " conv_lstm2d_2 (ConvLSTM2D)  (None, 6, 6, 64)             590080    ['encoder_input[0][0]']       \n",
            "                                                                                                  \n",
            " encoder_flatten (Flatten)   (None, 2304)                 0         ['conv_lstm2d_2[0][0]']       \n",
            "                                                                                                  \n",
            " repeat_vector_2 (RepeatVec  (None, 15, 2304)             0         ['encoder_flatten[0][0]']     \n",
            " tor)                                                                                             \n",
            "                                                                                                  \n",
            " decoder_input (InputLayer)  [(None, 15, 4)]              0         []                            \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 15, 2308)             0         ['repeat_vector_2[0][0]',     \n",
            " )                                                                   'decoder_input[0][0]']       \n",
            "                                                                                                  \n",
            " decoder_network (LSTM)      (None, 128)                  1247744   ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " decoder_dense (Dense)       (None, 1)                    129       ['decoder_network[0][0]']     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1837953 (7.01 MB)\n",
            "Trainable params: 1837953 (7.01 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "TRAINING: loss=['binary_crossentropy'] metrics=['accuracy']\n",
            "{'num_hidden': 128, 'reg_value': 0.001, 'activation': 'tanh', 'sequence_length': 15, 'lstm_dropout': 0.25, 'lstm_recurrent_dropout': 0.2, 'convlstm_num_filters': 64, 'convlstm_kernel_size': 2, 'encoder_input_size': 4, 'decoder_input_size': 4, 'decoder_dense_output_size': 1, 'encoder_seq_length': 15, 'decoder_seq_length': 15}\n",
            "Wrote configs to data/pie/intention/16May2024-12h12m20s/configs\n",
            "{'num_hidden': 128, 'reg_value': 0.001, 'activation': 'tanh', 'sequence_length': 15, 'lstm_dropout': 0.25, 'lstm_recurrent_dropout': 0.2, 'convlstm_num_filters': 64, 'convlstm_kernel_size': 2, 'encoder_input_size': 4, 'decoder_input_size': 4, 'decoder_dense_output_size': 1, 'encoder_seq_length': 15, 'decoder_seq_length': 15}\n",
            "Epoch 1/100\n",
            "37/37 [==============================] - 29s 663ms/step - loss: 1.5085 - accuracy: 0.6920 - val_loss: 1.1038 - val_accuracy: 0.7681 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "37/37 [==============================] - 29s 788ms/step - loss: 0.9831 - accuracy: 0.7022 - val_loss: 0.8519 - val_accuracy: 0.7681 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "37/37 [==============================] - 39s 1s/step - loss: 0.7775 - accuracy: 0.7352 - val_loss: 0.6725 - val_accuracy: 0.8250 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "37/37 [==============================] - 23s 635ms/step - loss: 0.5519 - accuracy: 0.8824 - val_loss: 0.7082 - val_accuracy: 0.8094 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "37/37 [==============================] - 23s 633ms/step - loss: 0.4350 - accuracy: 0.9475 - val_loss: 0.6208 - val_accuracy: 0.8551 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "37/37 [==============================] - 23s 635ms/step - loss: 0.3004 - accuracy: 0.9848 - val_loss: 0.7141 - val_accuracy: 0.8239 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "37/37 [==============================] - 23s 622ms/step - loss: 0.2854 - accuracy: 0.9755 - val_loss: 0.6963 - val_accuracy: 0.8417 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "37/37 [==============================] - 23s 629ms/step - loss: 0.2815 - accuracy: 0.9721 - val_loss: 0.8631 - val_accuracy: 0.8004 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "37/37 [==============================] - 24s 644ms/step - loss: 0.2559 - accuracy: 0.9687 - val_loss: 0.5819 - val_accuracy: 0.8317 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "37/37 [==============================] - 24s 641ms/step - loss: 0.2281 - accuracy: 0.9763 - val_loss: 0.7494 - val_accuracy: 0.8227 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "37/37 [==============================] - 23s 626ms/step - loss: 0.1553 - accuracy: 0.9890 - val_loss: 1.2827 - val_accuracy: 0.7681 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "37/37 [==============================] - 24s 641ms/step - loss: 0.1816 - accuracy: 0.9788 - val_loss: 0.7379 - val_accuracy: 0.8328 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "37/37 [==============================] - 23s 622ms/step - loss: 0.2216 - accuracy: 0.9679 - val_loss: 0.7405 - val_accuracy: 0.8060 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "37/37 [==============================] - ETA: 0s - loss: 0.1567 - accuracy: 0.9797\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "37/37 [==============================] - 24s 639ms/step - loss: 0.1567 - accuracy: 0.9797 - val_loss: 0.6312 - val_accuracy: 0.8484 - lr: 0.0010\n",
            "Epoch 14: early stopping\n",
            "Wrote configs to data/pie/intention/16May2024-12h12m20s/configs\n",
            "0.5\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " encoder_input (InputLayer)  [(None, 15, 7, 7, 512)]      0         []                            \n",
            "                                                                                                  \n",
            " conv_lstm2d_2 (ConvLSTM2D)  (None, 6, 6, 64)             590080    ['encoder_input[0][0]']       \n",
            "                                                                                                  \n",
            " encoder_flatten (Flatten)   (None, 2304)                 0         ['conv_lstm2d_2[0][0]']       \n",
            "                                                                                                  \n",
            " repeat_vector_2 (RepeatVec  (None, 15, 2304)             0         ['encoder_flatten[0][0]']     \n",
            " tor)                                                                                             \n",
            "                                                                                                  \n",
            " decoder_input (InputLayer)  [(None, 15, 4)]              0         []                            \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 15, 2308)             0         ['repeat_vector_2[0][0]',     \n",
            " )                                                                   'decoder_input[0][0]']       \n",
            "                                                                                                  \n",
            " decoder_network (LSTM)      (None, 128)                  1247744   ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " decoder_dense (Dense)       (None, 1)                    129       ['decoder_network[0][0]']     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1837953 (7.01 MB)\n",
            "Trainable params: 1837953 (7.01 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Generating test features crop_type=context crop_mode=pad_resize \n",
            "save_path=/content/drive/My Drive/Colab Notebooks/CV/PIE_data/data/data/pie/test/features_context_pad_resize/vgg16_none, \n",
            "151/151 [==============================] - 29s 187ms/step\n",
            "Generating test features crop_type=context crop_mode=pad_resize \n",
            "save_path=/content/drive/My Drive/Colab Notebooks/CV/PIE_data/data/data/pie/test/features_context_pad_resize/vgg16_none, \n",
            "111/111 [==============================] - 15s 131ms/step\n",
            "Generating test features crop_type=context crop_mode=pad_resize \n",
            "save_path=/content/drive/My Drive/Colab Notebooks/CV/PIE_data/data/data/pie/test/features_context_pad_resize/vgg16_none, \n",
            "78/78 [==============================] - 10s 130ms/step\n",
            "Generating test features crop_type=context crop_mode=pad_resize \n",
            "save_path=/content/drive/My Drive/Colab Notebooks/CV/PIE_data/data/data/pie/test/features_context_pad_resize/vgg16_none, \n",
            "41/41 [==============================] - 6s 137ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------------+\n",
            "| Intention model (local_context + bbox) |\n",
            "+-------------------+--------------------+\n",
            "|        Acc        |         F1         |\n",
            "+-------------------+--------------------+\n",
            "| 0.876980198019802 | 0.9324024119327199 |\n",
            "+-------------------+--------------------+\n"
          ]
        }
      ],
      "source": [
        "# @title Train_Test\n",
        "\n",
        "def train_intent():\n",
        "    data_opts = {'fstride': 1,\n",
        "            'sample_type': 'all',\n",
        "            'height_rng': [0, float('inf')],\n",
        "            'squarify_ratio': 0,\n",
        "            'data_split_type': 'default',\n",
        "            'seq_type': 'intention', #  crossing , intention\n",
        "            'min_track_size': 0, #  discard tracks that are shorter\n",
        "            'max_size_observe': 15,  # number of observation frames\n",
        "            'max_size_predict': 5,  # number of prediction frames\n",
        "            'seq_overlap_rate': 0.5,  # how much consecutive sequences overlap\n",
        "            'balance': False,  # balance the training and testing samples\n",
        "            'crop_type': 'context',  # crop 2x size of bbox around the pedestrian\n",
        "            'crop_mode': 'pad_resize',  # pad with 0s and resize to VGG input\n",
        "            'encoder_input_type': [],\n",
        "            'decoder_input_type': ['bbox'],\n",
        "            'output_type': ['intention_binary']}\n",
        "\n",
        "    t = PIEIntent(num_hidden_units=128,\n",
        "                  regularizer_val=0.001,\n",
        "                  lstm_dropout=0.25,\n",
        "                  lstm_recurrent_dropout=0.2,\n",
        "                  convlstm_num_filters=64,\n",
        "                  convlstm_kernel_size=2)\n",
        "    saved_files_path = ''\n",
        "    pretrained_model_path = '/content/drive/My Drive/Colab Notebooks/CV/PIE_data/data/pie/intention/context_loc_pretrained'\n",
        "        #beh_seq_train = imdb.balance_samples_count(beh_seq_train, label_type='intention_binary')\n",
        "    saved_files_path = t.train(data_train=train,\n",
        "                                   data_val=val,\n",
        "                                   epochs=100,\n",
        "                                   loss=['binary_crossentropy'],\n",
        "                                   metrics=['accuracy'],\n",
        "                                   batch_size=32,\n",
        "                                   optimizer_type='rmsprop',\n",
        "                                   data_opts=data_opts)\n",
        "    print(data_opts['seq_overlap_rate'])\n",
        "    if saved_files_path == '':\n",
        "            saved_files_path = pretrained_model_path\n",
        "    acc, f1 = t.test_chunk(test, data_opts, saved_files_path, True)\n",
        "    t = PrettyTable(['Acc', 'F1'])\n",
        "    t.title = 'Intention model (local_context + bbox)'\n",
        "    t.add_row([acc, f1])\n",
        "    print(t)\n",
        "    K.clear_session()\n",
        "    return saved_files_path\n",
        "\n",
        "intent_model_path = train_intent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "321_lE2UzAlb",
        "outputId": "4b06de69-a2ab-4b7f-a8fc-ab899eb5280e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['ped_id', 'images', 'results', 'gt'])\n"
          ]
        }
      ],
      "source": [
        "# @title save results in .txt\n",
        "with open('/content/data/pie/intention/16May2024-07h59m59s/ped_intents.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "print(data.keys())\n",
        "def ndarray_to_list(data):\n",
        "    if isinstance(data, np.ndarray):\n",
        "        return data.tolist()\n",
        "    elif isinstance(data, dict):\n",
        "        return {k: ndarray_to_list(v) for k, v in data.items()}\n",
        "    elif isinstance(data, list):\n",
        "        return [ndarray_to_list(x) for x in data]\n",
        "    else:\n",
        "        return data\n",
        "data = ndarray_to_list(data)\n",
        "data_str = json.dumps(data)\n",
        "with open('res.txt', 'w') as f:\n",
        "    f.write(data_str)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5o--3yDqTuTH"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}